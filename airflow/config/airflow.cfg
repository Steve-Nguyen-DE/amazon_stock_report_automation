[core]
# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor
executor = LocalExecutor

sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /opt/airflow/logs

# The default timezone that will be used by the airflow scheduler
# and web interface
default_timezone = utc

[database]
# The SQLAlchemy connection string to the metadata database.
# This connection string is used by all core components (scheduler, webserver, etc.).
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The connection string for the metadata database to be used for task logs.
# Leave it empty if you don't need to use a separate database for task logs.
#sql_alchemy_conn_task_log =

[logging]
# The airflow logging level
logging_level = INFO

# Specify the classname of the logging configuration to use
log_config_class = custom_logging_config.LOGGING_CONFIG

# Whether to print exception tracebacks when an error is logged
logging_exception_tracebacks = True

# Logging format for console output
#base_log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s

base_log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

[webserver]
# The base URL of the web interface
base_url = http://localhost:8080

# The IP address and port on which the web server should listen
web_server_host = 0.0.0.0
web_server_port = 8080

# The number of workers that should be available to the gunicorn web server
web_server_workers = 4

[cli]
# Whether to load the default connections defined in Airflow
load_default_connections = True

[scheduler]
scheduler_heartbeat_sec = 5
scheduler_health_check_threshold = 30